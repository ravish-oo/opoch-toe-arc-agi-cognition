# WO-1 — Π Ruler (Types)

## Anchors to read first

* `@docs/anchors/00_math_spec.md`: §3 Ruler Π, stencil definition, idempotence.
* `@docs/anchors/01_math_spec_addendum.md`: no changes to Π; confirms Π is free, idempotent, task-agnostic.

## Goal

Implement the fixed, idempotent Π typing on any grid (Y\in\mathcal C^{H\times W}). Π maps each pixel (p) to a **type id** by hashing the fixed local stencil (\phi(p)):

* center,
* 4-neighbors (N,E,S,W),
* 2-step neighbors (2N,2E,2S,2W),
* boundary flags ([r\in{0,H-1},, c\in{0,W-1}]),
* parity ((r\bmod2,,c\bmod2)),
  with sentinel = −1 outside.

Return both the type mosaic (T\in\mathbb N^{H\times W}) and a **codebook** mapping feature-tuples → type ids. Π **must** be idempotent.

## Interfaces (frozen)

```python
# arc/pi.py
from typing import Dict, Tuple
import numpy as np

def types_from_output(Y: np.ndarray) -> Tuple[np.ndarray, Dict[Tuple[int, ...], int]]:
    """Return (T, codebook) for the fixed Π stencil on Y."""
```

## Libraries and exact functions to use (no ad-hoc algorithms)

Use mature, documented NumPy primitives only:

* **Padding with sentinel −1**: `numpy.pad` with `mode='constant', constant_values=-1` to safely index N/E/S/W and 2-step neighbors without conditionals ([numpy.org][1]).
* **Stacking channels**: `numpy.stack` (or `numpy.column_stack` after reshape) to build the feature tensor from shifted views ([numpy.org][2]).
* **Vectorized hashing to ids**: `numpy.unique(..., return_inverse=True, axis=0)` to:

  * deduplicate feature rows → codebook,
  * get `inverse` to reconstruct per-pixel type ids without Python loops ([numpy.org][3]).
* **Stable hashes for receipts**: `hashlib.sha256` to hash codebooks and mosaics reproducibly; do **not** roll your own hashing ([Python documentation][4]).

> Implementation hint: you **don’t** need sliding windows. Pad once, then gather each neighbor plane by slicing (`padY[1:-1, 1:-1]`, `padY[0:-2,1:-1]`, …, `padY[1:-1,3:-1]`), stack 13 channels (center + 8 neighbors + 2 flags + 2 parities) into an `(H*W, F)` array, run `np.unique(axis=0, return_inverse=True)`, and reshape `inverse` back to `(H, W)`.

## Determinism requirements

* Row-major iteration; no RNG.
* Type ids come from NumPy’s `unique` inverse mapping; do **not** maintain a Python dict counter except for the returned `codebook` (which is derived from the unique values).
* The same input grid must yield identical `T` and `codebook_hash` across runs and machines.

## Receipts (first-class, emitted by this WO)

Emit **per training output and per test mosaic (when used later)**:

A1) **Π idempotence**

* Compute `T1 = Π(Y)`, then `T2 = Π_applied_again(Y)` (i.e., recompute on `Y`).
* Receipt: `sha256_T1`, `sha256_T2`, `pass_idempotent = (sha256_T1 == sha256_T2)`.

A2) **Codebook stability**

* Serialize feature tuples of unique rows in a stable order (e.g., lexicographic on the unique array rows), and compute `codebook_sha256`.
* Re-run Π on the same grid and confirm `codebook_sha256` unchanged.

A3) **Partition totals**

* Receipt: `type_sizes = {type_id: count}`, `sum_sizes = H*W`, `pass_sum = (sum_sizes == H*W)`.

(These invariants are sufficient to certify Π is implemented exactly as in the anchors, without ground truth.)

## Runner changes (minimal, to support reviewer from WO-1)

* Add a receipts skeleton in `arc/receipts.py` with helpers to write JSONL lines and SHA256.
* Add a **receipts-only** pass to the runner:

```bash
python -m arc.solve --mode pi-receipts --challenges /path/arc-agi_training_challenges.json --out outputs/receipts_wo1.jsonl
```

Behavior:

* Load every task; for each **training output grid Y** (and **test input** if you want to sanity-probe Π there too), call `types_from_output(Y)`.
* Write one JSONL object per grid:

  ```json
  {
    "task_id": "007bbfb7",
    "grid_role": "train_output",
    "grid_index": 0,
    "H": 9, "W": 9,
    "sha256_T": "...",
    "sha256_T_again": "...",
    "pass_idempotent": true,
    "codebook_sha256": "...",
    "type_sizes": {"0":81, "1":...},
    "sum_sizes": 81,
    "pass_sum": true
  }
  ```

No other modules are invoked yet.

## Reviewer instructions (run on all 1000, no debugging needed)

1. **Run** the receipts pass:

   ```bash
   python -m arc.solve --mode pi-receipts --challenges /mnt/data/arc-agi_training_challenges.json --out outputs/receipts_wo1.jsonl
   ```
2. **Assert**:

   * All JSONL lines show `pass_idempotent == true` and `pass_sum == true`.
   * For any given `(task_id, grid_role, grid_index)`, repeated runs produce identical `sha256_T` and `codebook_sha256`.
3. **Flagging a legitimate gap**:

   * There should be **no** legitimate “unsatisfiable” cases in WO-1; Π is purely local and must pass for every grid. Any failure indicates an implementation error (not a spec gap), e.g., wrong padding, wrong neighbor offsets, or nondeterministic codebook serialization.

## Performance guidance

* Pure NumPy, vectorized. `np.pad`, slicing, `np.stack`, `np.unique(axis=0, return_inverse=True)` are all O(HW) and CPU-friendly; no optimization hacks needed. Do **not** micro-optimize or prematurely compress features; correctness first.

## Acceptance criteria (green = WO-1 done)

* ✔ All 1000 tasks processed with receipts (no crashes).
* ✔ Every receipt: `pass_idempotent == true`, `pass_sum == true`.
* ✔ Re-running the pass yields identical `sha256_T` and `codebook_sha256` for every grid (determinism).
* ✔ No custom hashers, no Python loops over pixels (vectorized per above).

---

[1]: https://numpy.org/devdocs/reference/generated/numpy.pad.html?utm_source=chatgpt.com "numpy.pad — NumPy v2.4.dev0 Manual"
[2]: https://numpy.org/devdocs/reference/generated/numpy.stack.html?utm_source=chatgpt.com "numpy.stack — NumPy v2.4.dev0 Manual"
[3]: https://numpy.org/doc/stable/reference/generated/numpy.unique.html?utm_source=chatgpt.com "numpy.unique — NumPy v2.3 Manual"
[4]: https://docs.python.org/3/library/hashlib.html?utm_source=chatgpt.com "hashlib — Secure hashes and message digests"
